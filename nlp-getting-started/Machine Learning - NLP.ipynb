{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[38;5;1m✘ Can't locate model data\u001b[0m\r\n",
      "The data should be located in\r\n",
      "/Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages/spacy/spacymodels/en_core_web_md-2.3.0/en_core_web_md\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U spacy\n",
    "!python -m spacy link /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages/spacy/spacymodels/en_core_web_md-2.3.0/en_core_web_md en_core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 16.2 MB/s eta 0:00:01     |████████████████▍               | 12.4 MB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.0.0.tar.gz (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: requests in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.14.9-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 9.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.18.0,>=1.17.9\n",
      "  Downloading botocore-1.17.9-py2.py3-none-any.whl (6.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 16.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.9->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.0.0-py3-none-any.whl size=101341 sha256=110833e44e6f79d89ed77c618c905b2d8daf8c7a4f7b1020f40c7327dcb6e18d\n",
      "  Stored in directory: /Users/julesbarbosa/Library/Caches/pip/wheels/bb/1c/9c/412ec03f6d5ac7d41f4b965bde3fc0d1bd201da5ba3e2636de\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "Successfully installed boto3-1.14.9 botocore-1.17.9 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\r\n",
      "/Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages/en_core_web_md -->\r\n",
      "/Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages/spacy/data/en_core\r\n",
      "You can now load the model via spacy.load('en_core')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy link en_core_web_md en_core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.0/en_core_web_md-2.3.0.tar.gz (50.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.8 MB 15.0 MB/s eta 0:00:01   |▍                               | 675 kB 2.4 MB/s eta 0:00:21     |█████████████████████████       | 39.5 MB 7.1 MB/s eta 0:00:02     |█████████████████████████▎      | 40.1 MB 7.1 MB/s eta 0:00:02     |███████████████████████████████▍| 49.8 MB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from en_core_web_md==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (4.42.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.18.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.22.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (46.0.0.post20200309)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.2.0)\n",
      "Building wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-md: filename=en_core_web_md-2.3.0-py3-none-any.whl size=50921514 sha256=82416ba18b676884ba6265f8943d812874f0a9b63bcfe72384fdd5b505c5ad52\n",
      "  Stored in directory: /private/var/folders/gl/2wg4_bhn67jctgwgpms49tbr0000gn/T/pip-ephem-wheel-cache-kgnr3aek/wheels/a9/30/7d/40a0d13f1ddae5b6398c9f407391942152348eb9eae62fa21e\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: spacy link [-h] [-f] origin link_name [model_path]\r\n",
      "spacy link: error: the following arguments are required: origin, link_name\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "====================== Installed models (spaCy v2.3.0) ======================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages/spacy\u001b[0m\n",
      "\n",
      "TYPE      NAME             MODEL            VERSION                            \n",
      "package   en-core-web-sm   en_core_web_sm   \u001b[38;5;2m2.3.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "package   en-core-web-md   en_core_web_md   \u001b[38;5;2m2.3.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "link      en_core          en_core_web_md   \u001b[38;5;2m2.3.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import *\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import os\n",
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import ShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      target\n",
       "count   7613.000000  7613.00000\n",
       "mean    5441.934848     0.42966\n",
       "std     3137.116090     0.49506\n",
       "min        1.000000     0.00000\n",
       "25%     2734.000000     0.00000\n",
       "50%     5408.000000     0.00000\n",
       "75%     8146.000000     1.00000\n",
       "max    10873.000000     1.00000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/julesbarbosa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core', parse=True, tag=True, entity=True)\n",
    "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, #entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You all cannot expand contractions I would think'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "expand_contractions(\"Y'all can't expand contractions I'd think\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think Brasil'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? #Brasil\", \n",
    "                          remove_digits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sLemmatize keeps the semantic but is lower than stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash ! his crash yesterday , ours crash daily'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', , stopwords , computer not'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Forest fire near La Ronge Sask. Canada',\n",
       " 'clean_text': 'forest fire near la ronge sask canada'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-process text and store the same\n",
    "df['clean_text'] = normalize_corpus(df['text'], html_stripping=False)\n",
    "norm_corpus = list(df['clean_text'])\n",
    "\n",
    "# show a sample news article\n",
    "df.iloc[1][['text', 'clean_text']].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df[\"target\"]\n",
    "X= df.drop(\"target\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA and Vectorizer in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_train['clean_text'] = normalize_corpus(X_train['text'], html_stripping=False)\n",
    "norm_corpus_train = list(X_train['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6090"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(norm_corpus_train)\n",
    "X = vectorizer.transform(norm_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda = pd.DataFrame(data=lda.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 5)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(norm_corpus_train)\n",
    "tfidf_train = vectorizer.transform(norm_corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "vector_df = pd.DataFrame.sparse.from_spmatrix(tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 16345)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0a69394fe6cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_lda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_lda' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_vect = df_lda.merge(vector_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [False],\n",
      " 'max_depth': [80, 100],\n",
      " 'max_features': ['sqrt'],\n",
      " 'min_samples_leaf': [4],\n",
      " 'min_samples_split': [5],\n",
      " 'n_estimators': [400]}\n"
     ]
    }
   ],
   "source": [
    "# n_estimators\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
    "\n",
    "# max_features\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# max_depth\n",
    "max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# min_samples_split\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# min_samples_leaf\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# bootstrap\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': [400],\n",
    "               'max_features': [\"sqrt\"],\n",
    "               'max_depth': [80, 100],\n",
    "               'min_samples_split':[5],\n",
    "               'min_samples_leaf': [4],\n",
    "               'bootstrap': [False]}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 2 is smaller than n_iter=50. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                                                    oob_score=False,\n",
       "                                                    random_state=8, verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='deprecated', n_iter=50, n_jobs=None,\n",
       "                   param_distributions={'bootstrap': [False],\n",
       "                                        'max_depth': [80, 100],\n",
       "                                        'max_features': ['sqrt'],\n",
       "                                        'min_samples_leaf': [4],\n",
       "                                        'min_samples_split': [5],\n",
       "                                        'n_estimators': [400]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=8, refit=True,\n",
       "                   return_train_score=False, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# First create the base model to tune\n",
    "rfc = RandomForestClassifier(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=rfc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 100, 'bootstrap': False}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.7738916256157635\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfc = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=100, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=4, min_samples_split=5,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
       "                       n_jobs=None, oob_score=False, random_state=8, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rfc.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: \n",
      "0.8415435139573071\n"
     ]
    }
   ],
   "source": [
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(y_train, best_rfc.predict(X_train_vect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['clean_text'] = normalize_corpus(X_test['text'], html_stripping=False)\n",
    "norm_corpus_test = list(X_test['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(norm_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_test = pd.DataFrame(data=lda.transform(X_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vectorizer.transform(norm_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df_test = pd.DataFrame.sparse.from_spmatrix(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect = df_lda_test.merge(vector_df_test, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pred = best_rfc.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: \n",
      "0.7202889034799738\n"
     ]
    }
   ],
   "source": [
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.99      0.80       874\n",
      "           1       0.97      0.36      0.52       649\n",
      "\n",
      "    accuracy                           0.72      1523\n",
      "   macro avg       0.82      0.67      0.66      1523\n",
      "weighted avg       0.80      0.72      0.68      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report\")\n",
    "print(classification_report(y_test,rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USING ONLY LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julesbarbosa/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 2 is smaller than n_iter=50. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                                                    oob_score=False,\n",
       "                                                    random_state=8, verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='deprecated', n_iter=50, n_jobs=None,\n",
       "                   param_distributions={'bootstrap': [False],\n",
       "                                        'max_depth': [80, 100],\n",
       "                                        'max_features': ['sqrt'],\n",
       "                                        'min_samples_leaf': [4],\n",
       "                                        'min_samples_split': [5],\n",
       "                                        'n_estimators': [400]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=8, refit=True,\n",
       "                   return_train_score=False, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# First create the base model to tune\n",
    "rfc = RandomForestClassifier(random_state=8)\n",
    "\n",
    "# Definition of the random search\n",
    "random_search = RandomizedSearchCV(estimator=rfc,\n",
    "                                   param_distributions=random_grid,\n",
    "                                   n_iter=50,\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=3, \n",
    "                                   verbose=1, \n",
    "                                   random_state=8)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(vector_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters from Random Search are:\n",
      "{'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 80, 'bootstrap': False}\n",
      "\n",
      "The mean accuracy of a model with these hyperparameters is:\n",
      "0.629392446633826\n"
     ]
    }
   ],
   "source": [
    "print(\"The best hyperparameters from Random Search are:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"\")\n",
    "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pred = best_rfc.predict(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 921.6x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aux_df = df[['target']].drop_duplicates().sort_values('target')\n",
    "conf_matrix = confusion_matrix(y_test, rfc_pred)\n",
    "plt.figure(figsize=(12.8,6))\n",
    "sns.heatmap(conf_matrix, \n",
    "            annot=True,\n",
    "            xticklabels=aux_df['target'].values, \n",
    "            yticklabels=aux_df['target'].values,\n",
    "            cmap=\"Blues\")\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Actual')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to creat POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't lowercase to get POS context\n",
    "##sentence = str(df.iloc[1].text)\n",
    "##sentence_nlp = nlp(sentence)\n",
    "\n",
    "# POS tagging with Spacy \n",
    "##spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
    "##pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(La, 'FAC'), (Ronge, 'FAC'), (Sask, 'FAC'), (Canada, 'GPE')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Forest fire near \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    La Ronge Sask\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Canada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = str(df.iloc[1].text)\n",
    "sentence_nlp = nlp(sentence)\n",
    "\n",
    "# print named entities in article\n",
    "print([(word, word.ent_type_) for word in sentence_nlp if word.ent_type_])\n",
    "\n",
    "# visualize named entities\n",
    "displacy.render(sentence_nlp, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = []\n",
    "for sentence in df.text:\n",
    "    temp_entity_name = ''\n",
    "    temp_named_entity = None\n",
    "    sentence = nlp(sentence)\n",
    "    for word in sentence:\n",
    "        term = word.text \n",
    "        tag = word.ent_type_\n",
    "        if tag:\n",
    "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "            temp_named_entity = (temp_entity_name, tag)\n",
    "        else:\n",
    "            if temp_named_entity:\n",
    "                named_entities.append(temp_named_entity)\n",
    "                temp_entity_name = ''\n",
    "                temp_named_entity = None\n",
    "\n",
    "entity_frame = pd.DataFrame(named_entities, \n",
    "                            columns=['Entity Name', 'Entity Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Entity Name</th>\n",
       "      <td>#</td>\n",
       "      <td>2</td>\n",
       "      <td>Hiroshima</td>\n",
       "      <td>3</td>\n",
       "      <td>California</td>\n",
       "      <td>one</td>\n",
       "      <td>Legionnaires</td>\n",
       "      <td>today</td>\n",
       "      <td>first</td>\n",
       "      <td>Japan</td>\n",
       "      <td>two</td>\n",
       "      <td>Obama</td>\n",
       "      <td>US</td>\n",
       "      <td>Two</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entity Type</th>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>GPE</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>GPE</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>DATE</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>GPE</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>GPE</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency</th>\n",
       "      <td>554</td>\n",
       "      <td>112</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>40</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1          2         3           4         5  \\\n",
       "Entity Name         #         2  Hiroshima         3  California       one   \n",
       "Entity Type  CARDINAL  CARDINAL        GPE  CARDINAL         GPE  CARDINAL   \n",
       "Frequency         554       112         78        73          65        62   \n",
       "\n",
       "                        6      7        8      9        10      11   12  \\\n",
       "Entity Name  Legionnaires  today    first  Japan       two   Obama   US   \n",
       "Entity Type           ORG   DATE  ORDINAL    GPE  CARDINAL  PERSON  GPE   \n",
       "Frequency              58     56       50     48        45      40   34   \n",
       "\n",
       "                   13        14  \n",
       "Entity Name       Two  Malaysia  \n",
       "Entity Type  CARDINAL       GPE  \n",
       "Frequency          33        33  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
    "                           .size()\n",
    "                           .sort_values(ascending=False)\n",
    "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "top_entities.T.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Entity Type</th>\n",
       "      <td>ORG</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>GPE</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NORP</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>LOC</td>\n",
       "      <td>TIME</td>\n",
       "      <td>FAC</td>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>WORK_OF_ART</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>QUANTITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency</th>\n",
       "      <td>2559</td>\n",
       "      <td>1809</td>\n",
       "      <td>1632</td>\n",
       "      <td>1494</td>\n",
       "      <td>849</td>\n",
       "      <td>480</td>\n",
       "      <td>335</td>\n",
       "      <td>246</td>\n",
       "      <td>210</td>\n",
       "      <td>197</td>\n",
       "      <td>162</td>\n",
       "      <td>124</td>\n",
       "      <td>98</td>\n",
       "      <td>78</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1     2       3     4     5      6    7     8    9  \\\n",
       "Entity Type   ORG  CARDINAL   GPE  PERSON  DATE  NORP  MONEY  LOC  TIME  FAC   \n",
       "Frequency    2559      1809  1632    1494   849   480    335  246   210  197   \n",
       "\n",
       "                  10       11           12     13        14  \n",
       "Entity Type  PRODUCT  ORDINAL  WORK_OF_ART  EVENT  QUANTITY  \n",
       "Frequency        162      124           98     78        71  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_entities = (entity_frame.groupby(by=['Entity Type'])\n",
    "                           .size()\n",
    "                           .sort_values(ascending=False)\n",
    "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "top_entities.T.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=8, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(vector_df, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: \n",
      "0.9737274220032841\n"
     ]
    }
   ],
   "source": [
    "#train accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(y_train, svc.predict(vector_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: \n",
      "0.8003939592908733\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(y_test, svc.predict(vector_df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.84       874\n",
      "           1       0.81      0.69      0.75       649\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification revector_df_testport\n",
    "print(\"Classification report\")\n",
    "print(classification_report(y_test, svc.predict(vector_df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=8, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_all = svm.SVC(random_state=8)\n",
    "svc_all.fit(X_train_vect, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1523, 16350)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1523, 16350)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(y_train, svc_all.predict(X_train_vect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: \n",
      "0.7951411687458962\n"
     ]
    }
   ],
   "source": [
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(y_test, svc_all.predict(X_test_vect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=8, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=8)\n",
    "\n",
    "gbc.fit(vector_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: \n",
      "0.7922824302134647\n"
     ]
    }
   ],
   "source": [
    "#train accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(y_train, gbc.predict(vector_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUBMITION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['clean_text'] = normalize_corpus(df_test['text'], html_stripping=False)\n",
    "norm_corpus_sub = list(df_test['clean_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sub = vectorizer.transform(norm_corpus_sub)\n",
    "#df_lda_sub = pd.DataFrame(data=lda.transform(df_sub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_sub = vectorizer.transform(norm_corpus_sub)\n",
    "vector_df_sub = pd.DataFrame.sparse.from_spmatrix(tfidf_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_sub_vect = df_lda_sub.merge(vector_df_sub, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = svc.predict(vector_df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = pd.DataFrame(data=submition )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition[\"id\"] = df_test[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition.rename(columns={0: \"target\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target', 'id']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(submition.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'target']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = submition.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub = submition[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>hear earthquake different city stay safe everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond goose flee across street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>earthquake safety los angeles uo safety fasten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>storm ri bad last hurricane cityampother hard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>green line derailment chicago httpt coutbxlcbiuy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>meg issue hazardous weather outlook hwo httpt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>cityofcalgary activate municipal emergency pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  \\\n",
       "0                    Just happened a terrible car crash   \n",
       "1     Heard about #earthquake is different cities, s...   \n",
       "2     there is a forest fire at spot pond, geese are...   \n",
       "3              Apocalypse lighting. #Spokane #wildfires   \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "...                                                 ...   \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n",
       "3259  Storm in RI worse than last hurricane. My city...   \n",
       "3260  Green Line derailment in Chicago http://t.co/U...   \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
       "3262  #CityofCalgary has activated its Municipal Eme...   \n",
       "\n",
       "                                             clean_text  \n",
       "0                             happen terrible car crash  \n",
       "1     hear earthquake different city stay safe everyone  \n",
       "2     forest fire spot pond goose flee across street...  \n",
       "3                  apocalypse lighting spokane wildfire  \n",
       "4                    typhoon soudelor kill china taiwan  \n",
       "...                                                 ...  \n",
       "3258  earthquake safety los angeles uo safety fasten...  \n",
       "3259  storm ri bad last hurricane cityampother hard ...  \n",
       "3260   green line derailment chicago httpt coutbxlcbiuy  \n",
       "3261  meg issue hazardous weather outlook hwo httpt ...  \n",
       "3262  cityofcalgary activate municipal emergency pla...  \n",
       "\n",
       "[3263 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
